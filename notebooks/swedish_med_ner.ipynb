{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swedish medical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinyan/opt/anaconda3/envs/saman_med_ner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 309k/309k [00:00<00:00, 669kB/s]\n",
      "Downloading data: 100%|██████████| 67.1k/67.1k [00:00<00:00, 404kB/s]\n",
      "Downloading data: 100%|██████████| 116k/116k [00:00<00:00, 478kB/s]\n",
      "Generating train split: 100%|██████████| 3394/3394 [00:00<00:00, 216370.80 examples/s]\n",
      "Generating validation split: 100%|██████████| 1009/1009 [00:00<00:00, 142034.26 examples/s]\n",
      "Generating test split: 100%|██████████| 1287/1287 [00:00<00:00, 231686.74 examples/s]\n",
      "Downloading data: 100%|██████████| 2.57M/2.57M [00:01<00:00, 1.41MB/s]\n",
      "Generating train split: 100%|██████████| 48720/48720 [00:00<00:00, 486480.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "wnut['train'][0]\n",
    "#    dataset w  bad formatting\n",
    "ner_dataset = load_dataset('swedish_medical_ner', 'wiki')\n",
    "ner_dataset['train'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sid': ['wiki_0',\n",
       "  'wiki_1',\n",
       "  'wiki_2',\n",
       "  'wiki_3',\n",
       "  'wiki_4',\n",
       "  'wiki_5',\n",
       "  'wiki_6',\n",
       "  'wiki_7',\n",
       "  'wiki_8',\n",
       "  'wiki_9'],\n",
       " 'sentence': ['{kropp} beskrivs i till exempel människokroppen, anatomi och f',\n",
       "  'sju miljoner år gammalt hominint {kranium}, klassificerad som ',\n",
       "  ' autosomer och ett par könskromosomer. Varje {kromosom} består',\n",
       "  ' {kromosom} består av en DNA-molekyl och {protein}. En DNA-molek',\n",
       "  'tikel:Människans {skelett} Människans skelett\\xa0är det\\xa0skelett\\xa0s',\n",
       "  'os\\xa0människor. En vuxen människas {skelett} består av 206 till ',\n",
       "  '{lett} består av 206 till 220 {ben}, beroende på hur man räknar.',\n",
       "  'v kroppsvikten.Ett nyfött barn har ca 300 {ben} i kroppen vilk',\n",
       "  'kollektivet i mindre bitar såsom länder > städer > orter {Hud}',\n",
       "  'sdjur. {Huden} utgör ett mekaniskt skydd mot omvärlden och bid'],\n",
       " 'entities': [{'start': [0], 'end': [7], 'text': ['kropp'], 'type': [2]},\n",
       "  {'start': [33], 'end': [42], 'text': ['kranium'], 'type': [2]},\n",
       "  {'start': [45], 'end': [55], 'text': ['kromosom'], 'type': [2]},\n",
       "  {'start': [1],\n",
       "   'end': [50],\n",
       "   'text': ['kromosom} består av en DNA-molekyl och {protein'],\n",
       "   'type': [2]},\n",
       "  {'start': [17], 'end': [26], 'text': ['skelett'], 'type': [2]},\n",
       "  {'start': [33], 'end': [42], 'text': ['skelett'], 'type': [2]},\n",
       "  {'start': [0],\n",
       "   'end': [35],\n",
       "   'text': ['lett} består av 206 till 220 {ben'],\n",
       "   'type': [2]},\n",
       "  {'start': [42], 'end': [47], 'text': ['ben'], 'type': [2]},\n",
       "  {'start': [57], 'end': [62], 'text': ['Hud'], 'type': [2]},\n",
       "  {'start': [7], 'end': [14], 'text': ['Huden'], 'type': [2]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sid\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m ner_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m100\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(entity)\n\u001b[1;32m      6\u001b[0m         all_entities\u001b[38;5;241m.\u001b[39madd(entity)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "all_entities = set()\n",
    "for row in ner_dataset['train'][:100]:\n",
    "    print(row)\n",
    "    for entity in row['entities']['text']:\n",
    "        print(entity)\n",
    "        all_entities.add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many entities have more than one word\n",
    "entities_with_more_than_one_word = []\n",
    "for entity in all_entities:\n",
    "    if len(entity.split()) > 1:\n",
    "        entities_with_more_than_one_word.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kant}. Mellan {dura ',\n",
       " 'akut stressyndrom',\n",
       " 'som halotan',\n",
       " 'ovopulmon Novolizer',\n",
       " 'glatta muskulaturen} i {aorta',\n",
       " 'sympatiska nervsystemet}, beroende på om kroppen eller {psy',\n",
       " 'skt stressyndrom',\n",
       " 'n STADA',\n",
       " 'aponeuros} täcker {Musculus rectus abdominis',\n",
       " 'kromosom} består av en DNA-molekyl och {protein']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_with_more_than_one_word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform dataset to BIO format for huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['tikel', ':', 'Människans', 'skelett', 'Människans', 'skelett', 'är', 'det', 'skelett', 's'], 'ner_tags': [0, 0, 0, 5, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,  # Outside of a named entity\n",
    "    \"B-DISORDER\": 1,\n",
    "    \"I-DISORDER\": 2,\n",
    "    \"B-DRUG\": 3,\n",
    "    \"I-DRUG\": 4,\n",
    "    \"B-BODY\": 5,\n",
    "    \"I-BODY\": 6,\n",
    "}\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-DISORDER\",\n",
    "    2: \"I-DISORDER\",\n",
    "    3: \"B-DRUG\",\n",
    "    4: \"I-DRUG\",\n",
    "    5: \"B-BODY\",\n",
    "    6: \"I-BODY\",\n",
    "    \n",
    "}\n",
    "def clean_space_characters(text):\n",
    "    return text.replace(u'\\xa0', u' ')\n",
    "\n",
    "def transform_dataset_row(sentence, nlp):\n",
    "    sentence = clean_space_characters(sentence)\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    current_entity_type = None\n",
    "    start_of_entity = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text == '(':\n",
    "            current_entity_type = (\"B-DISORDER\", \"I-DISORDER\")\n",
    "            start_of_entity = True\n",
    "        elif token.text == '[':\n",
    "            current_entity_type = (\"B-DRUG\", \"I-DRUG\")\n",
    "            start_of_entity = True\n",
    "        elif token.text == '{':\n",
    "            current_entity_type = (\"B-BODY\", \"I-BODY\")\n",
    "            start_of_entity = True\n",
    "        elif token.text in [')', ']', '}'] and current_entity_type:\n",
    "            current_entity_type = None\n",
    "        else:\n",
    "            if current_entity_type:\n",
    "                if start_of_entity == True:\n",
    "                    ner_tags.append(NER_TAGS[current_entity_type[0]])\n",
    "                    start_of_entity = False\n",
    "                else:\n",
    "                    ner_tags.append(NER_TAGS[current_entity_type[1]])\n",
    "            else:\n",
    "                ner_tags.append(NER_TAGS[\"O\"])\n",
    "            tokens.append(token.text)\n",
    "\n",
    "    return {'tokens': tokens, 'ner_tags': ner_tags}\n",
    "\n",
    "# Example usage\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "sentence = 'tikel:Människans {skelett} Människans skelett\\xa0är det\\xa0skelett\\xa0s'\n",
    "output = transform_dataset_row(sentence,nlp)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ner_dataset to a new format\n",
    "from datasets import Dataset\n",
    "new_train_dataset = []\n",
    "for row in ner_dataset['train']:\n",
    "    new_train_dataset.append(transform_dataset_row(row['sentence'],nlp))\n",
    "dataset = Dataset.from_list(new_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['kropp',\n",
       "  'beskrivs',\n",
       "  'i',\n",
       "  'till',\n",
       "  'exempel',\n",
       "  'människokroppen',\n",
       "  ',',\n",
       "  'anatomi',\n",
       "  'och',\n",
       "  'f'],\n",
       " 'ner_tags': [5, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43848, 4872)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForMaskedLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForMaskedLM\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKBLab/bert-base-swedish-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKBLab/bert-base-swedish-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/saman_med_ner/lib/python3.10/site-packages/transformers/utils/import_utils.py:1412\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1412\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/saman_med_ner/lib/python3.10/site-packages/transformers/utils/import_utils.py:1400\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1398\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForMaskedLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "model_id = 'KB/bert-base-swedish-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, tokens_get_same_label_as_first_token=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Special tokens get -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # First token of a word gets its label.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # Subsequent tokens of a word get the same label as the first token of the word.\n",
    "                if tokens_get_same_label_as_first_token:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_list = label2id.keys()\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"swe_medical_ner\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saman_med_ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
